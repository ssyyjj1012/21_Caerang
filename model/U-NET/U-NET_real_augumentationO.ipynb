{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6mkS2K2_9Lf"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL \n",
    "import urllib\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from random import uniform\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZ8g4tZL7F03"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as td\n",
    "import torchvision as tv\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "-X34L5UwIemg",
    "outputId": "93c46484-4e65-43aa-c409-6f8d70943956"
   },
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tALkYOd_7MGH",
    "outputId": "bfa78048-0981-4f99-fb3a-53196e10c3cc"
   },
   "outputs": [],
   "source": [
    "!pip install pydicom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCqEeHYC7ISq"
   },
   "outputs": [],
   "source": [
    "import pydicom as dcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V4c7UOjt7ybJ",
    "outputId": "f8bfc7c0-4775-40f7-be7b-729f7eea9399"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount._DEBUG = True\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0LeWnrf7pxM"
   },
   "outputs": [],
   "source": [
    "os.chdir('./drive/MyDrive/CapDi/Kidney/train/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQG8RrO1RPN7"
   },
   "source": [
    "전처리\n",
    "\n",
    "\n",
    "*   https://github.com/tuvovan/Unet-with-EfficientnetB7-Backbone/blob/master/Body%20Morphometry.ipynb\n",
    "\n",
    "\n",
    "모델\n",
    "\n",
    "\n",
    "*   https://github.com/IanTaehoonYoo/semantic-segmentation-pytorch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "EmiTRmsl9PzI",
    "outputId": "96d7006e-e879-4a9a-e2d2-374f52547ab2"
   },
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OL4PsCAWIlot"
   },
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "import numpy as np\n",
    "import pydicom\n",
    "\n",
    "def transform_to_hu(medical_image, image):\n",
    "    hu_image = image * medical_image.RescaleSlope + medical_image.RescaleIntercept\n",
    "    hu_image[hu_image < -1024] = -1024\n",
    "    return hu_image\n",
    "\n",
    "def window_image(image, window_center, window_width):\n",
    "    window_image = image.copy()\n",
    "    image_min = window_center - (window_width / 2)\n",
    "    image_max = window_center + (window_width / 2)\n",
    "    window_image[window_image < image_min] = image_min\n",
    "    window_image[window_image > image_max] = image_max\n",
    "    return window_image\n",
    "\n",
    "def resize_normalize(image):\n",
    "    image = np.array(image, dtype=np.float64)\n",
    "    image -= np.min(image)\n",
    "    image /= np.max(image)\n",
    "    return image\n",
    "\n",
    "def read_dicom(image_medical, window_widht, window_level):\n",
    "    image_data = image_medical.pixel_array\n",
    "\n",
    "    image_hu = transform_to_hu(image_medical, image_data)\n",
    "    image_window = window_image(image_hu.copy(), window_level, window_widht)\n",
    "    image_window_norm = resize_normalize(image_window)\n",
    "#     image_window_norm = image_window\n",
    "\n",
    "    image_window_norm = np.expand_dims(image_window_norm, axis=2)   # (512, 512, 1)\n",
    "    image_ths = np.concatenate([image_window_norm, image_window_norm, image_window_norm], axis=2)   # (512, 512, 3)\n",
    "    #print(image_window_norm.shape)\n",
    "    return image_ths\n",
    "\n",
    "def to_binary(img, lower, upper):\n",
    "    return (lower <= img) & (img <= upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYRisXzBI22p"
   },
   "outputs": [],
   "source": [
    "def mask_binarization(mask, threshold=None):\n",
    "    if threshold is None:\n",
    "        threshold = 0.5\n",
    "\n",
    "    if isinstance(mask, np.ndarray):\n",
    "        mask_binarized = (mask > threshold).astype(np.uint8)\n",
    "    \n",
    "    elif isinstance(mask, torch.Tensor):\n",
    "        zeros = torch.zeros_like(mask)\n",
    "        ones = torch.ones_like(mask)\n",
    "        \n",
    "        mask_binarized = torch.where(mask > threshold, ones, zeros)\n",
    "    \n",
    "    return mask_binarized\n",
    "\n",
    "def augment_imgs_and_masks(imgs, masks, rot_factor, scale_factor, trans_factor, flip):\n",
    "    rot_factor = uniform(-rot_factor, rot_factor)\n",
    "    ran_alp = uniform(10,100)\n",
    "    scale_factor = uniform(1-scale_factor, 1+scale_factor)\n",
    "    trans_factor = [int(imgs.shape[1]*uniform(-trans_factor, trans_factor)),\n",
    "                    int(imgs.shape[2]*uniform(-trans_factor, trans_factor))]\n",
    "\n",
    "    seq = iaa.Sequential([\n",
    "            iaa.Affine(\n",
    "                translate_px={\"x\": trans_factor[0], \"y\": trans_factor[1]},\n",
    "                scale=(scale_factor, scale_factor),\n",
    "                rotate=rot_factor\n",
    "            ),\n",
    "            #iaa.ElasticTransformation(alpha=ran_alp,sigma=5.0)\n",
    "        \n",
    "        ])\n",
    "\n",
    "    seq_det = seq.to_deterministic()\n",
    "\n",
    "    imgs = seq_det.augment_images(imgs)\n",
    "    masks = seq_det.augment_images(masks)\n",
    "\n",
    "    if flip and uniform(0, 1) > 0.5:\n",
    "        imgs = np.flip(imgs, 2).copy()\n",
    "        masks = np.flip(masks, 2).copy()\n",
    "    \n",
    "    masks = mask_binarization(masks).astype(np.float32)\n",
    "    return imgs, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARXf726QI22q"
   },
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "rot_factor = 45. \n",
    "scale_factor = 0.15\n",
    "flip = False\n",
    "trans_factor = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ou_vStef7Kut"
   },
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_dir, y_dir,augmentation=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.x_img = x_dir\n",
    "        self.y_img = y_dir   \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_img)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_img = self.x_img[idx]\n",
    "        y_img = self.y_img[idx]\n",
    "        # Read an image with OpenCV\n",
    "        x_img = dcm.read_file(x_img)\n",
    "        y_img =  imread(y_img)\n",
    "\n",
    "        x_img=read_dicom(x_img,400,0)\n",
    "        x_img=np.transpose(x_img,(2,0,1))\n",
    "        x_img=x_img.astype(np.float32)\n",
    "\n",
    "        y_img = resize(y_img, (512, 512))*255\n",
    "        color_im = np.zeros([512, 512, 2])\n",
    "        for i in range(1,3):\n",
    "            encode_ = to_binary(y_img, i*1.0, i*1.0)\n",
    "            color_im[:, :, i-1] = encode_\n",
    "        color_im = np.transpose(color_im,(2,0,1))\n",
    "        # Data Augmentation\n",
    "        if self.augmentation:\n",
    "            img, mask = augment_imgs_and_masks(x_img, color_im, rot_factor, scale_factor, trans_factor, flip)\n",
    "        \n",
    "        return img,mask,y_img\n",
    "#         if self.transforms:\n",
    "#             augmented = self.transforms(image=x_img,mask=color_im)\n",
    "#             img = augmented['image']\n",
    "#             mask = augmented['mask']\n",
    "#             return img, mask,y_img\n",
    "# #         return x_img,color_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MK5j3S645aVz"
   },
   "outputs": [],
   "source": [
    "data_path_folder=sorted(os.listdir(\"./train/DICOM/\")) \n",
    "label_path_folder=sorted(os.listdir(\"./train/Label/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LuECHhxG4jO2"
   },
   "outputs": [],
   "source": [
    "#case 겹치지 않게 train,val 나누기\n",
    "import glob\n",
    "test_input_files=[]\n",
    "test_label_files=[]\n",
    "val_input_files=[]\n",
    "val_label_files=[]\n",
    "train_input_files=[]\n",
    "train_label_files=[]\n",
    "\n",
    "for i in range(100):\n",
    "  if i<70:\n",
    "    train_input_files+=sorted(glob.glob(\"./train/DICOM/\"+data_path_folder[i]+\"/*.dcm\",recursive=True))\n",
    "    train_label_files+=sorted(glob.glob(\"./train/Label/\"+label_path_folder[i]+\"/*.png\",recursive=True))\n",
    "  elif i<90:\n",
    "    val_input_files+=sorted(glob.glob(\"./train/DICOM/\"+data_path_folder[i]+\"/*.dcm\",recursive=True))\n",
    "    val_label_files+=sorted(glob.glob(\"./train/Label/\"+label_path_folder[i]+\"/*.png\",recursive=True))\n",
    "  else:  \n",
    "    test_input_files+=sorted(glob.glob(\"./train/DICOM/\"+data_path_folder[i]+\"/*.dcm\",recursive=True))\n",
    "    test_label_files+=sorted(glob.glob(\"./train/Label/\"+label_path_folder[i]+\"/*.png\",recursive=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7JE1IDH8iTl"
   },
   "outputs": [],
   "source": [
    "train_input_files = np.array(train_input_files)\n",
    "train_label_files = np.array(train_label_files)\n",
    "\n",
    "val_input_files = np.array(val_input_files)\n",
    "val_label_files = np.array(val_label_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDn5LKmJTiM1",
    "outputId": "dbc8d628-f87d-48bc-df99-dd94fdafa14b"
   },
   "outputs": [],
   "source": [
    "len(val_input_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4isFVrb_8weM"
   },
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_input_files,train_label_files)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2,shuffle=True)\n",
    "val_dataset = MyDataset(val_input_files,val_label_files)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "1MJmXHLz88_3",
    "outputId": "3f529d85-9eb5-407f-9583-400f911bb679",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##input과 label이 맞나 확인\n",
    "images,labels,a = next(iter(train_loader))\n",
    "# print(images.shape)\n",
    "# print(labels.shape)\n",
    "print(labels[labels>1])\n",
    "plt.figure(figsize=(16,18))\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(images[0][0],cmap='gray')\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(labels[0][0])\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(labels[0][1])\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(a[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RPu17l8trAm"
   },
   "outputs": [],
   "source": [
    "def compute_per_channel_dice(input, target, epsilon=1e-5,ignore_index=None, weight=None):\n",
    "    # assumes that input is a normalized probability\n",
    "    # input and target shapes must match\n",
    "    assert input.size() == target.size(), \"'input' and 'target' must have the same shape\"\n",
    "\n",
    "    # mask ignore_index if present\n",
    "    if ignore_index is not None:\n",
    "        mask = target.clone().ne_(ignore_index)\n",
    "        mask.requires_grad = False\n",
    "\n",
    "        input = input * mask\n",
    "        target = target * mask\n",
    "\n",
    "    input = flatten(input)\n",
    "    target = flatten(target)\n",
    "\n",
    "    # Compute per channel Dice Coefficient\n",
    "    intersect = (input * target).sum(-1)\n",
    "    if weight is not None:\n",
    "        intersect = weight * intersect\n",
    "\n",
    "    denominator = (input + target).sum(-1)\n",
    "    return 2. * intersect / denominator.clamp(min=epsilon)\n",
    "\n",
    "def flatten(tensor):\n",
    "    \"\"\"Flattens a given tensor such that the channel axis is first.\n",
    "    The shapes are transformed as follows:\n",
    "       (N, C, D, H, W) -> (C, N * D * H * W)\n",
    "    \"\"\"\n",
    "    C = tensor.size(1)\n",
    "    # new axis order\n",
    "    axis_order = (1, 0) + tuple(range(2, tensor.dim()))\n",
    "    # Transpose: (N, C, D, H, W) -> (C, N, D, H, W)\n",
    "    transposed = tensor.permute(axis_order).contiguous()\n",
    "    # Flatten: (C, N, D, H, W) -> (C, N * D * H * W)\n",
    "    return transposed.view(C, -1)\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Computes Dice Loss, which just 1 - DiceCoefficient described above.\n",
    "    Additionally allows per-class weights to be provided.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=1e-5, weight=None, ignore_index=None, sigmoid_normalization=True,\n",
    "                 skip_last_target=False):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        if isinstance(weight, list):\n",
    "            weight = torch.Tensor(weight)\n",
    "            \n",
    "        self.epsilon = epsilon\n",
    "        self.register_buffer('weight', weight)\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "        if sigmoid_normalization:\n",
    "            self.normalization = nn.Sigmoid()\n",
    "        else:\n",
    "            self.normalization = nn.Softmax(dim=1)\n",
    "        # if True skip the last channel in the target\n",
    "        self.skip_last_target = skip_last_target\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # get probabilities from logits\n",
    "\n",
    "        input = self.normalization(input)\n",
    "        if self.weight is not None:\n",
    "            weight = Variable(self.weight, requires_grad=False).to(input.device)\n",
    "        else:\n",
    "            weight = None\n",
    "\n",
    "        if self.skip_last_target:\n",
    "            target = target[:, :-1, ...]\n",
    "\n",
    "        per_channel_dice = compute_per_channel_dice(input, target, epsilon=self.epsilon, ignore_index=self.ignore_index, weight=weight)\n",
    "        # Average the Dice score across all channels/classes\n",
    "        return torch.mean(1. - per_channel_dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CT1XuWj1fm4O"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        def CBR2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True):\n",
    "            layers = []\n",
    "            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                 kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                 bias=bias)]\n",
    "            layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "            layers += [nn.ReLU()]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)\n",
    "\n",
    "            return cbr\n",
    "\n",
    "\n",
    "        self.enc1_1 = CBR2d(in_channels=3, out_channels=128)\n",
    "        self.enc1_2 = CBR2d(in_channels=128, out_channels=128)\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.enc2_1 = CBR2d(in_channels=128, out_channels=256)\n",
    "        self.enc2_2 = CBR2d(in_channels=256, out_channels=256)\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.enc3_1 = CBR2d(in_channels=256, out_channels=512)\n",
    "        self.enc3_2 = CBR2d(in_channels=512, out_channels=512)\n",
    "\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.enc4_1 = CBR2d(in_channels=512, out_channels=1024)\n",
    "        self.enc4_2 = CBR2d(in_channels=1024, out_channels=1024)\n",
    "\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.enc5_1 = CBR2d(in_channels=1024, out_channels=2048)\n",
    "        \n",
    "\n",
    "        self.dec5_1 = CBR2d(in_channels=2048, out_channels=1024)\n",
    "\n",
    "        self.unpool4 = nn.ConvTranspose2d(in_channels=1024, out_channels=1024,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "\n",
    "        self.dec4_2 = CBR2d(in_channels=2 * 1024, out_channels=1024)\n",
    "        self.dec4_1 = CBR2d(in_channels=1024, out_channels=512)\n",
    "\n",
    "        self.unpool3 = nn.ConvTranspose2d(in_channels=512, out_channels=512,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "\n",
    "        self.dec3_2 = CBR2d(in_channels=2 * 512, out_channels=512)\n",
    "        self.dec3_1 = CBR2d(in_channels=512, out_channels=256)\n",
    "\n",
    "        self.unpool2 = nn.ConvTranspose2d(in_channels=256, out_channels=256,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "\n",
    "        self.dec2_2 = CBR2d(in_channels=2 * 256, out_channels=256)\n",
    "        self.dec2_1 = CBR2d(in_channels=256, out_channels=128)\n",
    "\n",
    "        self.unpool1 = nn.ConvTranspose2d(in_channels=128, out_channels=128,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "\n",
    "        self.dec1_2 = CBR2d(in_channels=2 * 128, out_channels=128)\n",
    "        self.dec1_1 = CBR2d(in_channels=128, out_channels=128)\n",
    "\n",
    "        self.fc = nn.Conv2d(in_channels=128, out_channels=2, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "\n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "\n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2)\n",
    "\n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "\n",
    "        enc5_1 = self.enc5_1(pool4)\n",
    "        dec5_1 = self.dec5_1(enc5_1)\n",
    "\n",
    "        unpool4 = self.unpool4(dec5_1)\n",
    "        cat4 = torch.cat((unpool4, enc4_2), dim=1)\n",
    "        dec4_2 = self.dec4_2(cat4)\n",
    "        dec4_1 = self.dec4_1(dec4_2)\n",
    "\n",
    "        unpool3 = self.unpool3(dec4_1)\n",
    "        cat3 = torch.cat((unpool3, enc3_2), dim=1)\n",
    "        dec3_2 = self.dec3_2(cat3)\n",
    "        dec3_1 = self.dec3_1(dec3_2)\n",
    "\n",
    "        unpool2 = self.unpool2(dec3_1)\n",
    "        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n",
    "        dec2_2 = self.dec2_2(cat2)\n",
    "        dec2_1 = self.dec2_1(dec2_2)\n",
    "\n",
    "        unpool1 = self.unpool1(dec2_1)\n",
    "        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n",
    "        dec1_2 = self.dec1_2(cat1)\n",
    "        dec1_1 = self.dec1_1(dec1_2)\n",
    "\n",
    "        x = self.fc(dec1_1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JqIBpujq56m6"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix  \n",
    " #mport numpy as np\n",
    "\n",
    "def compute_iou(y_pred, y_true):\n",
    "    y_pred=y_pred.detach().cpu()\n",
    "    y_true=y_true.detach().cpu()\n",
    "    # ytrue, ypred is a flatten vector\n",
    "    y_pred = y_pred.flatten()\n",
    "    y_true = y_true.flatten()\n",
    "    current = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    # compute mean iou\n",
    "    intersection = np.diag(current)\n",
    "    ground_truth_set = current.sum(axis=1)\n",
    "    predicted_set = current.sum(axis=0)\n",
    "    union = ground_truth_set + predicted_set - intersection\n",
    "    IoU = intersection / union.astype(np.float32)\n",
    "    return np.mean(IoU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TiRaw4B39P1e"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion =  DiceLoss(sigmoid_normalization=True)\n",
    "model = UNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = F.binary_cross_entropy_with_logits(logits,labels)\n",
    "# criterion = torch.nn.BCEWithLogitsLoss(logits, labels)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yeCbq_D5I22s",
    "outputId": "52a116fb-e9f8-488e-cb42-516c4b1407e2"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eb7basNdI22s",
    "outputId": "5264a5e8-8a19-4abc-eb85-f6391334e71e"
   },
   "outputs": [],
   "source": [
    "sum([param.nelement() for param in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254,
     "referenced_widgets": [
      "8f556779baa84f65b570793716022aae",
      "bca048c802ce415db7cce002deaa9c50",
      "47f283337ca94b148ef6a6dbdd8f6634",
      "7689f69fcde5411b8e6ecbff2efa71ad",
      "d28f8cd1db584f26b122694b6b23f0fa",
      "5d9604daadb1494f9619afc87d992e0d",
      "a475d636bc5b4f7f8dd833e474a46e31",
      "5edc16d82d0444ba996155021c340919",
      "87326bcea45b490ea7255138d2676b21",
      "19913c50b70849489eef24e8247bc5bb",
      "5be1d743a1f7463aa0ecb64d19184cb1"
     ]
    },
    "id": "BxFM0Rno9xtV",
    "outputId": "5fddc324-fc81-4a2e-f1bd-7f4d1c9d2dbe"
   },
   "outputs": [],
   "source": [
    "n_epochs = 100 \n",
    "cnt = 0\n",
    "valid_loss_min = np.inf\n",
    "\n",
    "# keep track of training and validation loss\n",
    "train_loss = torch.zeros(n_epochs)\n",
    "valid_loss = torch.zeros(n_epochs)\n",
    "Iou=0\n",
    "model.to(device)\n",
    "for e in range(0, n_epochs):\n",
    "\n",
    "   \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, labels,a in tqdm(train_loader):\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        data, labels = data.to(device), labels.to(device) #cpu에 있는 데이터를 gpu에 보냄\n",
    "        # clear the gradients of all optimized variables\n",
    "#         print(data.shape)\n",
    "#         break\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        logits = model(data)\n",
    "        \n",
    "        # calculate the batch loss\n",
    "        loss = criterion(logits, labels)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss[e] += loss.item()\n",
    "        \n",
    "        \n",
    "#         z=logits.detach().cpu().numpy()\n",
    "#         z = z.astype(np.uint8)\n",
    "        cnt = cnt+1\n",
    "        \n",
    "        \n",
    "        if cnt %1000==0:\n",
    "            \n",
    "            logits = logits.sigmoid()\n",
    "            logits = mask_binarization(logits.detach().cpu(), 0.5)\n",
    "            iou = compute_iou(logits,labels)\n",
    "            print(iou)\n",
    "            # y=torch.squeeze(labels[0])\n",
    "            y=logits[0].detach().cpu().numpy()\n",
    "            # x=data[0].detach().cpu().numpy()\n",
    "            x=labels[0].detach().cpu().numpy()\n",
    "            #y=labels[0].numpy()\n",
    "            plt.figure(figsize=(16,18))\n",
    "            plt.subplot(1,5,1)\n",
    "            plt.imshow(x[0])\n",
    "            plt.subplot(1,5,2)\n",
    "            plt.imshow(x[1])\n",
    "            plt.subplot(1,5,3)\n",
    "            plt.imshow(y[0])\n",
    "            plt.subplot(1,5,4)\n",
    "            plt.imshow(y[1])\n",
    "            plt.subplot(1,5,5)\n",
    "            plt.imshow(a[0])\n",
    "            plt.show()\n",
    "\n",
    "    \n",
    "    train_loss[e] /= len(train_loader)\n",
    "    #torch.save(model.state_dict(), 'model_.pt')\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    with torch.no_grad(): \n",
    "        model.eval()\n",
    "        for data, labels,a in tqdm(val_loader):\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            logits = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(logits, labels)\n",
    "            # update average validation loss \n",
    "            valid_loss[e] += loss.item()\n",
    "\n",
    "    \n",
    "    # calculate average losses\n",
    "    valid_loss[e] /= len(val_loader)\n",
    "    scheduler.step(valid_loss[e])    \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        e, train_loss[e], valid_loss[e]))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss[e] <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss[e]))\n",
    "        torch.save(model.state_dict(), 'model_best_2.pt')\n",
    "        valid_loss_min = valid_loss[e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5pC9vPXQ30y"
   },
   "outputs": [],
   "source": [
    "#Loss\n",
    "plt.plot(train_loss)\n",
    "plt.plot(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-kyvSyrk_gP7"
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('model_.pt'))\n",
    "model.load_state_dict(torch.load('model_best_2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7tjs6fZDF1nP"
   },
   "outputs": [],
   "source": [
    "len(test_input_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-aiH1HO_3fI"
   },
   "outputs": [],
   "source": [
    "class TestMyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_dir,y_dir,augmentation = False):\n",
    "        super().__init__()\n",
    "        self.augmentation = augmentation\n",
    "        self.x_img = x_dir\n",
    "        self.y_img = y_dir\n",
    "     \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_img)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_img = self.x_img[idx]\n",
    "        y_img = self.y_img[idx]\n",
    "        # Read an image with OpenCV\n",
    "        if x_img[-1]=='m' or y_img[-1]=='g':         \n",
    "            x_img = dcm.read_file(x_img)\n",
    "            x_img=read_dicom(x_img,400,50)\n",
    "            x_img=np.transpose(x_img,(2,0,1))\n",
    "            x_img=x_img.astype(np.float32)\n",
    "        else:\n",
    "            x_img = np.load(x_img)\n",
    "            x_img=resize_normalize(x_img)\n",
    "            y_img = np.load(y_img)\n",
    "\n",
    "        image_window_norm = np.expand_dims(x_img, axis=2)   # (512, 512, 1)\n",
    "        x_img = np.concatenate([image_window_norm, image_window_norm, image_window_norm], axis=2)   # (512, 512, 3)\n",
    "        x_img=np.transpose(x_img,(2,0,1))\n",
    "        x_img=x_img.astype(np.float32)\n",
    "        \n",
    "        y_img = y_img\n",
    "        color_im = np.zeros([512, 512, 2])\n",
    "        for i in range(1,3):\n",
    "            encode_ = to_binary(y_img, i*1.0, i*1.0) * 255\n",
    "            color_im[:, :, i-1] = encode_\n",
    "        color_im = np.transpose(color_im,(2,0,1))\n",
    "        # Data Augmentation\n",
    "        if self.augmentation:\n",
    "            img, mask = augment_imgs_and_masks(x_img, color_im, rot_factor, scale_factor, trans_factor, flip)\n",
    "\n",
    "        return x_img,color_im,y_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IuULlvWU4Hd-"
   },
   "outputs": [],
   "source": [
    "test_dataset = TestMyDataset(test_input_files,test_label_files)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2D3WKq4uxEwv"
   },
   "outputs": [],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3_Ig2EjI22v"
   },
   "outputs": [],
   "source": [
    "images,labels,a = next(iter(test_loader))\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "plt.figure(figsize=(16,18))\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(images[0][0])\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(labels[0][0])\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(labels[0][1])\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(a[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cADTTLy3Gcod"
   },
   "outputs": [],
   "source": [
    "cnt =0\n",
    "Iou=0\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad(): \n",
    "        model.eval()\n",
    "        for data, labels,a in tqdm(test_loader):\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                logits = model(data)\n",
    "                logits = logits.sigmoid()\n",
    "                logits = mask_binarization(logits.detach().cpu(), 0.5)\n",
    "                iouu = compute_iou(logits,labels)\n",
    "                iouu=np.round(iouu,3)*100\n",
    "                if np.isnan(iouu)==True:\n",
    "                    iouu=100\n",
    "                Iou+=iouu\n",
    "\n",
    "                labels=labels[0].detach().cpu().numpy()\n",
    "                logits=logits[0].detach().cpu().numpy()\n",
    "                cnt = cnt+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aA7mWturo0pV"
   },
   "outputs": [],
   "source": [
    "print(\"Iou:\",Iou/len(test_loader))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "check (4).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "19913c50b70849489eef24e8247bc5bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "47f283337ca94b148ef6a6dbdd8f6634": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a475d636bc5b4f7f8dd833e474a46e31",
      "placeholder": "​",
      "style": "IPY_MODEL_5d9604daadb1494f9619afc87d992e0d",
      "value": " 45%"
     }
    },
    "5be1d743a1f7463aa0ecb64d19184cb1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d9604daadb1494f9619afc87d992e0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5edc16d82d0444ba996155021c340919": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7689f69fcde5411b8e6ecbff2efa71ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87326bcea45b490ea7255138d2676b21",
      "max": 2240,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5edc16d82d0444ba996155021c340919",
      "value": 1011
     }
    },
    "87326bcea45b490ea7255138d2676b21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f556779baa84f65b570793716022aae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_47f283337ca94b148ef6a6dbdd8f6634",
       "IPY_MODEL_7689f69fcde5411b8e6ecbff2efa71ad",
       "IPY_MODEL_d28f8cd1db584f26b122694b6b23f0fa"
      ],
      "layout": "IPY_MODEL_bca048c802ce415db7cce002deaa9c50"
     }
    },
    "a475d636bc5b4f7f8dd833e474a46e31": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bca048c802ce415db7cce002deaa9c50": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d28f8cd1db584f26b122694b6b23f0fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5be1d743a1f7463aa0ecb64d19184cb1",
      "placeholder": "​",
      "style": "IPY_MODEL_19913c50b70849489eef24e8247bc5bb",
      "value": " 1011/2240 [29:46&lt;35:06,  1.71s/it]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
